{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lateb\\OneDrive\\Desktop\\CODING\\PharmacyProblemAnalyzer-Gemini-main\\PharmacyProblemAnalyzer-Gemini-main\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Lateb\\OneDrive\\Desktop\\CODING\\PharmacyProblemAnalyzer-Gemini-main\\PharmacyProblemAnalyzer-Gemini-main\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "model.save('sentence-transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.float16, low_cpu_mem_usage=True)\n",
    "\n",
    "tokenizer.save_pretrained(\"tinyllama-tokenizer\")\n",
    "model.save_pretrained(\"tinyllama-model\", max_shard_size=\"1000MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 3\n",
      "[Document(id_='37bd3c49-466d-4628-a612-5a9ead4f1562', embedding=None, metadata={'file_path': 'c:\\\\Users\\\\Lateb\\\\OneDrive\\\\Desktop\\\\CODING\\\\PharmacyProblemAnalyzer-Gemini-main\\\\PharmacyProblemAnalyzer-Gemini-main\\\\sample_files\\\\sample1.txt', 'file_name': 'sample1.txt', 'file_type': 'text/plain', 'file_size': 229, 'creation_date': '2024-05-19', 'last_modified_date': '2024-05-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"The issues with the pharmacy are as follows: 1. The cashier doesn't have an electronic billing machine so customer service is extremely slow. 2. The medicine is not labeled or arranged in order so searching for it is very tiring.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f45b9ecb-97ca-49af-ac10-b8b368c00835', embedding=None, metadata={'file_path': 'c:\\\\Users\\\\Lateb\\\\OneDrive\\\\Desktop\\\\CODING\\\\PharmacyProblemAnalyzer-Gemini-main\\\\PharmacyProblemAnalyzer-Gemini-main\\\\sample_files\\\\sample2.txt', 'file_name': 'sample2.txt', 'file_type': 'text/plain', 'file_size': 391, 'creation_date': '2024-05-19', 'last_modified_date': '2024-05-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Medication Adherence:Non-adherence: Patients failing to follow medication schedules can lead to treatment failure and potential health complications.Complexity of Regimens: Complicated dosing schedules or pill combinations can lead to confusion and missed medications.Lack of Patient Education: Inadequate understanding of medication purpose and side effects can contribute to non-adherence.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='12f99d76-5ee5-418a-ab7c-1156a6f26cec', embedding=None, metadata={'file_path': 'c:\\\\Users\\\\Lateb\\\\OneDrive\\\\Desktop\\\\CODING\\\\PharmacyProblemAnalyzer-Gemini-main\\\\PharmacyProblemAnalyzer-Gemini-main\\\\sample_files\\\\sample3.txt', 'file_name': 'sample3.txt', 'file_type': 'text/plain', 'file_size': 338, 'creation_date': '2024-05-19', 'last_modified_date': '2024-05-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Long Wait Times: Understaffing: Insufficient pharmacy staff can lead to long waiting times for patients seeking medication or consultation. Repetitive Tasks: Manual processes and excessive paperwork can slow down pharmacists and technicians. Inefficient Workflow: Poorly designed workflow can create bottlenecks and hinder timely service.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n",
      "Loaded 3 chunks from './sample_files/'\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "data_dir = \"./sample_files/\"\n",
    "\n",
    "docs = SimpleDirectoryReader(\n",
    "        input_dir=data_dir\n",
    ").load_data()\n",
    "\n",
    "print('Number of pages:', len(docs))\n",
    "print(docs)\n",
    "\n",
    "print (f\"Loaded {len(docs)} chunks from '{data_dir}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "API_KEY = 'AIzaSyA3xX1ZaJGwFO2KNwS3wR6oj4ZCMM9xkX0'\n",
    "uri = \"mongodb+srv://geminiuser:1234@cluster0.nurmebz.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "google_api_key = API_KEY\n",
    "os.environ[\"GOOGLE_API_KEY\"] = google_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas client initialized\n",
      "768\n",
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lateb\\AppData\\Local\\Temp\\ipykernel_22244\\2038043006.py:21: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import ServiceContext\n",
    "\n",
    "DB_NAME = \"langchain_demo\"\n",
    "COLLECTION_NAME = 'collection_of_text_blobs'\n",
    "INDEX_NAME = 'Indexx'\n",
    "\n",
    "\n",
    "mongodb_client = pymongo.MongoClient(uri)\n",
    "print (\"Atlas client initialized\")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"./sentence-transformers\"\n",
    ")\n",
    "\n",
    "vector = embed_model.get_text_embedding(\"Vector Search with MongoDB\")\n",
    "print(len(vector))\n",
    "\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n",
    "\n",
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(mongodb_client = mongodb_client,\n",
    "                                 db_name = DB_NAME, collection_name = COLLECTION_NAME,\n",
    "                                 index_name  = INDEX_NAME)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas client initialized\n",
      "768\n",
      "LLM is explicitly disabled. Using MockLLM.\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lateb\\AppData\\Local\\Temp\\ipykernel_22244\\4290373620.py:29: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n",
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.29.235:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug:127.0.0.1 - - [24/May/2024 02:27:25] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "INFO:werkzeug:127.0.0.1 - - [24/May/2024 02:27:26] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "INFO:werkzeug:192.168.29.235 - - [24/May/2024 02:27:28] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "INFO:werkzeug:192.168.29.235 - - [24/May/2024 02:27:29] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 chunks from uploaded files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [24/May/2024 02:31:10] \"POST /upload HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "#Api to upload files onto mongodb\n",
    "from flask import Flask, request, jsonify\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pymongo\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "DB_NAME = \"langchain_demo\"\n",
    "COLLECTION_NAME = 'collection_of_text_blobs'\n",
    "INDEX_NAME = 'Indexx'\n",
    "mongodb_client = pymongo.MongoClient(uri)\n",
    "db = mongodb_client[DB_NAME]\n",
    "collection = db[COLLECTION_NAME]\n",
    "\n",
    "print(\"Atlas client initialized\")\n",
    "\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"./sentence-transformers\")\n",
    "\n",
    "vector = embed_model.get_text_embedding(\"Vector Search with MongoDB\")\n",
    "print(len(vector))\n",
    "\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n",
    "\n",
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(mongodb_client = mongodb_client,\n",
    "                                 db_name = DB_NAME, collection_name = COLLECTION_NAME,\n",
    "                                 index_name  = INDEX_NAME)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "\n",
    "@app.route('/upload', methods=['POST'])\n",
    "def upload_documents():\n",
    "    temp_dir = \"./temp_files\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    files = request.files.getlist(\"files\")\n",
    "    for file in files:\n",
    "        file_path = os.path.join(temp_dir, file.filename)\n",
    "        file.save(file_path)\n",
    "\n",
    "    docs = SimpleDirectoryReader(input_dir=temp_dir).load_data()\n",
    "    print(f\"Loaded {len(docs)} chunks from uploaded files.\")\n",
    "\n",
    "    index = VectorStoreIndex.from_documents(docs, storage_context=storage_context, service_context=service_context)\n",
    "\n",
    "    for doc in docs:\n",
    "        embedding = embed_model.get_text_embedding(doc.text)\n",
    "        collection.insert_one({\"text\": doc.text, \"embedding\": embedding})\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "    return jsonify({\"message\": f\"Successfully loaded {len(docs)} documents into MongoDB.\"}), 200\n",
    "\n",
    "def generate_embedding(text):\n",
    "    return embed_model.get_text_embedding(text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 chunks from './sample_files/'\n"
     ]
    }
   ],
   "source": [
    "#Embeds + uploads context files to mongoDB\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "data_dir = \"./sample_files/\"\n",
    "\n",
    "docs = SimpleDirectoryReader(\n",
    "        input_dir=data_dir\n",
    ").load_data()\n",
    "\n",
    "print (f\"Loaded {len(docs)} chunks from '{data_dir}'\")\n",
    "\n",
    "\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs, \n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document count before delete : 6\n",
      "Deleted docs : 6\n"
     ]
    }
   ],
   "source": [
    "#Delete embedding files to add new context into the db\n",
    "database = mongodb_client[DB_NAME]\n",
    "collection = database[COLLECTION_NAME]\n",
    "\n",
    "doc_count = collection.count_documents (filter = {})\n",
    "print (f\"Document count before delete : {doc_count:,}\")\n",
    "\n",
    "result = collection.delete_many(filter= {})\n",
    "print (f\"Deleted docs : {result.deleted_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    context_window=2048,\n",
    "    max_new_tokens=512,\n",
    "    generate_kwargs={\"temperature\": 0.1, \"do_sample\": False},\n",
    "    tokenizer_name=\"tinyllama-tokenizer\",\n",
    "    model_name=\"tinyllama-model\",\n",
    "    tokenizer_kwargs={\"max_length\": 2048},\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, clear_output, display\n",
    "import pprint\n",
    "\n",
    "stored_index = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context)\n",
    "\n",
    "query_engine = stored_index.as_query_engine()\n",
    "prompt=\"Patients failing to follow medication schedules can lead to what?\"\n",
    "response = query_engine.query(prompt)\n",
    "\n",
    "\n",
    "from IPython.display import Markdown, clear_output, display\n",
    "\n",
    "\n",
    "query = \"what are some problems with the pharmacy? Give me solutions to deal with it\"\n",
    "\n",
    "\n",
    "def generate_embedding(quer):\n",
    "  temp = embed_model.get_text_embedding(quer)\n",
    "  return temp\n",
    "\n",
    "mongodb_client = pymongo.MongoClient(uri)\n",
    "client = pymongo.MongoClient(uri)\n",
    "db = client.langchain_demo\n",
    "collection = db.collection_of_text_blobs\n",
    "\n",
    "results = collection.aggregate([\n",
    "  {\n",
    "    \"$vectorSearch\": {\n",
    "      \"queryVector\": generate_embedding(query),\n",
    "      \"path\": \"embedding\",\n",
    "      \"numCandidates\": 50,\n",
    "      \"limit\": 1,\n",
    "      \"index\": \"RAGIndexing\",\n",
    "    }\n",
    "  }\n",
    "])\n",
    "\n",
    "context = \"\"\n",
    "for document in results:\n",
    "    print(type(document))\n",
    "    print(f'Text present: {document[\"text\"]}\\n')\n",
    "    context += document[\"text\"]\n",
    "\n",
    "print(context)\n",
    "\n",
    "query_engine = context.as_query_engine()\n",
    "\n",
    "response = model.query([context, query], stream=True)\n",
    "\n",
    "buffer = []\n",
    "for chunk in response:\n",
    "    for part in chunk.parts:\n",
    "        buffer.append(part.text)\n",
    "    clear_output()\n",
    "    display(Markdown(''.join(buffer)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.29.235:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n",
      "<class 'dict'>\n",
      "Text present: Long Wait Times: Understaffing: Insufficient pharmacy staff can lead to long waiting times for patients seeking medication or consultation. Repetitive Tasks: Manual processes and excessive paperwork can slow down pharmacists and technicians. Inefficient Workflow: Poorly designed workflow can create bottlenecks and hinder timely service.\n",
      "\n",
      "Long Wait Times: Understaffing: Insufficient pharmacy staff can lead to long waiting times for patients seeking medication or consultation. Repetitive Tasks: Manual processes and excessive paperwork can slow down pharmacists and technicians. Inefficient Workflow: Poorly designed workflow can create bottlenecks and hinder timely service.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [24/May/2024 02:44:16] \"POST /query HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pymongo\n",
    "from IPython.display import Markdown, clear_output, display\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/query', methods=['POST'])\n",
    "def query_model():\n",
    "    query_data = request.json\n",
    "    query = query_data.get(\"query\")\n",
    "    \n",
    "    tokenizer_path = \"tinyllama-tokenizer\"  \n",
    "    model_path = \"tinyllama-model\"  \n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)\n",
    "        print(\"Model and tokenizer loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model and tokenizer: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "    def query_model(prompt: str) -> str:\n",
    "        try:\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=2048, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids=inputs.input_ids,\n",
    "                    attention_mask=inputs.attention_mask,\n",
    "                    max_new_tokens=512,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    do_sample=True\n",
    "                )\n",
    "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            return f\"Error during query processing: {e}\"\n",
    "\n",
    "    client = pymongo.MongoClient(uri)\n",
    "    db = client.langchain_demo\n",
    "    collection = db.collection_of_text_blobs\n",
    "\n",
    "    query = \"what are some problems with the pharmacy? Give me solutions to deal with it\"\n",
    "\n",
    "    def generate_embedding(quer):\n",
    "        temp = embed_model.get_text_embedding(quer)\n",
    "        return temp\n",
    "\n",
    "    results = collection.aggregate([\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"queryVector\": generate_embedding(query),\n",
    "                \"path\": \"embedding\",\n",
    "                \"numCandidates\": 50,\n",
    "                \"limit\": 1,\n",
    "                \"index\": \"RAGIndexing\",\n",
    "            }\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    context = \"\"\n",
    "    for document in results:\n",
    "        print(type(document))\n",
    "        print(f'Text present: {document[\"text\"]}\\n')\n",
    "        context += document[\"text\"]\n",
    "\n",
    "    print(context)\n",
    "\n",
    "    response = query_model(f\"{context}\\n\\n{query}\")\n",
    "    return jsonify({\"response\": response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
